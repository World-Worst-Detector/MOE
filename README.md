# MOE
## OUTRAGEOUSLY LARGE NEURAL NETWORKS:THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER
This paper introduced a the general idea of MOE in NLP.
The concept is simple to understand with expert and gating network in MOE.
<img width="677" alt="Screen Shot 2021-10-19 at 1 41 33 PM" src="https://user-images.githubusercontent.com/42342410/137962894-25b370f4-c463-41a9-944f-161ed7203577.png">
